{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown",
   "metadata": {},
   "source": [
    "# Upload NAT Calculator Traces to DBNL\n",
    "\n",
    "This notebook uploads OTEL traces from the NAT calculator agent to DBNL for analysis.\n",
    "\n",
    "**Prerequisites:**\n",
    "- DBNL sandbox running (`dbnl sandbox start`)\n",
    "- Trace files generated via `run_batch.py` or downloaded from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e530359-10ec-43d4-a6cc-7ff3dc378033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dbnl\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import UTC, datetime, timedelta\n",
    "import numpy as np\n",
    "import random\n",
    "from nat_otel_converter import dbnl_df_from_otel_file\n",
    "from data_augment_helpers import compute_expected_answer, compute_feedback, compute_abs_error, est_cost_from_llm_tokens\n",
    "\n",
    "print(\"dbnl version:\", dbnl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0306de-20c2-4c64-af5e-041ad06a777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to DBNL (using default Sandbox url)\n",
    "app_url = \"http://localhost:8080/\"\n",
    "dbnl.login(\n",
    "    api_url=\"http://localhost:8080/api\",\n",
    "    api_token=\"<DBNL_API_KEY>\", # found at http://localhost:8080/tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba715c9-eafa-4a20-8da9-efe55bc3aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project\n",
    "project = dbnl.get_or_create_project(\n",
    "    name=\"NAT Calculator HPO Example\",\n",
    "    schedule=\"daily\",\n",
    "    default_llm_model_name=\"quickstart_model\" # From step (2) in quickstart\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def break_into_days(df, start_day, num_days, variation=0.10):\n",
    "    \"\"\"\n",
    "    Split dataframe into daily chunks with Â±variation in size.\n",
    "    Ensures all rows are used exactly once.\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    base = total_rows / num_days\n",
    "    sizes = []\n",
    "    for _ in range(num_days):\n",
    "        factor = 1 + np.random.uniform(-variation, variation)\n",
    "        sizes.append(int(base * factor))\n",
    "\n",
    "    scale = total_rows / sum(sizes)\n",
    "    sizes = [int(s * scale) for s in sizes]\n",
    "    sizes[-1] = total_rows - sum(sizes[:-1])\n",
    "\n",
    "    day_dfs = []\n",
    "    idx = 0\n",
    "    for day_idx, size in enumerate(sizes):\n",
    "        chunk = df.iloc[idx : idx + size].copy()\n",
    "        idx += size\n",
    "\n",
    "        chunk[\"timestamp\"] = start_day.replace(hour=12) + timedelta(days=day_idx)\n",
    "\n",
    "        for row in chunk.itertuples(index=True):\n",
    "            chunk.at[row.Index, \"timestamp\"] = randomize_timestamps_within_day(row)\n",
    "            \n",
    "        day_dfs.append(chunk.reset_index(drop=True))\n",
    "\n",
    "    return day_dfs\n",
    "\n",
    "def randomize_timestamps_within_day(row):\n",
    "    rand_hour = random.randint(0, 23)\n",
    "    rand_min = random.randint(0, 59)\n",
    "    rand_sec = random.randint(0, 59)\n",
    "    return row.timestamp.replace(\n",
    "                hour=rand_hour,\n",
    "                minute=rand_min,\n",
    "                second=rand_sec,\n",
    "    )\n",
    "\n",
    "def upload_to_dbnl(day_dfs):\n",
    "    \"\"\"Upload daily dataframes to DBNL.\"\"\"\n",
    "    print(f\"Status: {app_url}/ns/{project.namespace_id}/projects/{project.id}/status\")\n",
    "    for idx, day_df in enumerate(day_dfs):\n",
    "        print(f\"{idx + 1}/{len(day_dfs)} uploading {min(day_df['timestamp']).date()} : {len(day_df)} traces.\")\n",
    "        data_start = min(day_df['timestamp']).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        data_end = data_start + timedelta(days=1)\n",
    "        try:\n",
    "            dbnl.log(\n",
    "                project_id=project.id,\n",
    "                data_start_time=data_start,\n",
    "                data_end_time=data_end,\n",
    "                data=day_df,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"Data already exists\" in str(e):\n",
    "                print(\"  Data already exists, skipping...\")\n",
    "                continue\n",
    "            raise\n",
    "    print(f\"\\nExplore: {app_url}/ns/{project.namespace_id}/projects/{project.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v0-section",
   "metadata": {},
   "source": [
    "## Upload v0 Traces (Pre-Optimization)\n",
    "\n",
    "These traces have `hyper_error_term=1.0` which causes errors on calculations with larger numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-v0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, augment, and upload v0 traces\n",
    "df_v0 = dbnl_df_from_otel_file(\"traces_v0.jsonl\")\n",
    "print(f\"Loaded {len(df_v0)} v0 traces\")\n",
    "\n",
    "df_v0[\"output_expected\"] = df_v0[\"input\"].apply(compute_expected_answer)\n",
    "df_v0[[\"feedback_score\", \"feedback_text\"]] = df_v0.apply(compute_feedback, axis=1)\n",
    "df_v0[\"absolute_error\"] = df_v0.apply(compute_abs_error, axis=1)\n",
    "df_v0[\"total_cost\"] = df_v0[\"traces_data\"].apply(est_cost_from_llm_tokens)\n",
    "df_v0[\"agent_version\"] = json.dumps(\"v0\")\n",
    "\n",
    "# Upload as 8 days of data, ending 8 days ago\n",
    "day_dfs_v0 = break_into_days(df_v0, start_day=datetime.now(tz=UTC) - timedelta(days=16), num_days=8)\n",
    "upload_to_dbnl(day_dfs_v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v1-section",
   "metadata": {},
   "source": [
    "## Upload v1 Traces (Post-Optimization)\n",
    "\n",
    "These traces have `hyper_error_term=0.0` (optimized value) which fixes the calculation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b55f94-a388-414a-b82c-da51a212a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, augment, and upload v1 traces\n",
    "df_v1 = dbnl_df_from_otel_file(\"traces_v1.jsonl\")\n",
    "print(f\"Loaded {len(df_v1)} v1 traces\")\n",
    "\n",
    "df_v1[\"output_expected\"] = df_v1[\"input\"].apply(compute_expected_answer)\n",
    "df_v1[[\"feedback_score\", \"feedback_text\"]] = df_v1.apply(compute_feedback, axis=1)\n",
    "df_v1[\"absolute_error\"] = df_v1.apply(compute_abs_error, axis=1)\n",
    "df_v1[\"total_cost\"] = df_v1[\"traces_data\"].apply(est_cost_from_llm_tokens)\n",
    "df_v0[\"agent_version\"] = json.dumps(\"v1\")\n",
    "\n",
    "# Upload as 8 days of data, ending 8 days ago\n",
    "day_dfs_v1 = break_into_days(df_v1, start_day=datetime.now(tz=UTC) - timedelta(days=8), num_days=8)\n",
    "upload_to_dbnl(day_dfs_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-section",
   "metadata": {},
   "source": [
    "## View Results in DBNL\n",
    "\n",
    "In the DBNL dashboard, you should see:\n",
    "- **v0 period**: Higher `absolute_error` values, especially for calculations with larger numbers\n",
    "- **v1 period**: Near-zero `absolute_error` across all calculations\n",
    "\n",
    "The `agent_version` field lets you segment by version to compare performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
